{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Classification project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "# import nltk\n",
    "#nltk.download()\n",
    "# import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pprint\n",
    "import tweepy\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.preprocessing.text as kpt\n",
    "\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Pre Prccessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/gender-classifier-DFE-791531.csv'\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + list(string.punctuation) + ['rt', 'via', '__']\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "    # r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    # r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    s = re.sub(r'[^\\x00-\\x7f]*',r'',str(s))\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def cleanAndNormalizeText(data):\n",
    "    tokens = tokenize(data)\n",
    "    tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    # remove stop words\n",
    "    filterText = [w for w in tokens if w not in stop]\n",
    "    filterText = [w for w in filterText if not len(w) <= 1]\n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(filterText) - 1):\n",
    "        if len(filterText[i]) > 1:\n",
    "            try:\n",
    "                filterText[i] = ps.stem(filterText[i])\n",
    "            except Exception as e:\n",
    "                filterText[i] = filterText[i]\n",
    "\n",
    "    return filterText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get learning data from gender-classifier-DFE-791531.csv file.\n",
    "\n",
    "for every tweet clean the text and the description and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "terms_male = []\n",
    "terms_female = []\n",
    "terms_brand = []\n",
    "genderClassData = []\n",
    "with open(path,'r', encoding=\"utf8\" ,errors=\"ignore\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count != 0:\n",
    "            text = row[10]+' '+row[19]\n",
    "            locations.append(row[25])\n",
    "            cleantext = cleanAndNormalizeText(text)\n",
    "            genderClassData.append([row[0], row[5], ' '.join(cleantext)])\n",
    "            for token in cleantext:\n",
    "                if token not in stop:\n",
    "                    if row[5] == 'male':\n",
    "                        terms_male.append(token)\n",
    "                    else:\n",
    "                        if row[5] == 'female':\n",
    "                            terms_female.append(token)\n",
    "                        else:\n",
    "                            if row[5] == 'brand':\n",
    "                                terms_brand.append(token)\n",
    "        count = count+1\n",
    "\n",
    "\n",
    "Table = pd.DataFrame(genderClassData, columns=['ID', 'gender', 'text'])\n",
    "Table = Table.loc[Table['gender'].isin(['female', 'male', 'brand'])]\n",
    "Table.loc[Table['text'].isnull(), 'text'] = \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender  # of Samples\n0    male          6194\n1  female          6700\n2   brand          5942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'gender')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class distribution\n",
    "classDistribution = []\n",
    "for c in ['male', 'female', 'brand']:\n",
    "    classDistribution.append([c, Table[\"gender\"].value_counts()[c]])\n",
    "classDis = pd.DataFrame(classDistribution, columns=['Gender', '# of Samples'])\n",
    "print(classDis)\n",
    "counter = Counter(Table['gender'])\n",
    "targetNames = counter.keys()\n",
    "tweetCounts = counter.values()\n",
    "indexes = np.arange(len(targetNames))\n",
    "width = 0.7\n",
    "plt.bar(indexes, tweetCounts, width)\n",
    "plt.xticks(indexes + width * 0.5, targetNames)\n",
    "print('Class distributions summary:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the popular location and term frequency for the diffrent genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male most common: 106680\n[('love', 668),\n ('like', 575),\n ('get', 558),\n ('one', 423),\n ('go', 409),\n ('time', 400),\n ('make', 398),\n ('life', 367),\n ('fan', 335),\n ('follow', 331),\n ('day', 313),\n ('know', 290),\n ('new', 286),\n ('good', 276),\n ('music', 275),\n ('peopl', 273),\n ('live', 271),\n ('game', 265),\n ('see', 257),\n ('work', 254),\n ('want', 251),\n ('year', 250),\n ('thing', 249),\n ('look', 246),\n ('man', 241),\n ('think', 235),\n ('best', 228),\n ('tweet', 225),\n ('world', 223),\n ('got', 218),\n ('come', 218),\n ('back', 215),\n ('need', 215),\n ('play', 211),\n ('sport', 209),\n ('fuck', 208),\n ('say', 204),\n ('writer', 187),\n ('im', 185),\n ('would', 181),\n ('still', 180),\n ('also', 175),\n ('way', 172),\n ('alway', 171),\n ('last', 170),\n ('god', 168),\n ('thank', 168),\n ('take', 166),\n ('book', 165),\n ('never', 165)]\nFemale most common: 102188\n[('love', 973),\n ('like', 711),\n ('get', 555),\n ('one', 508),\n ('make', 503),\n ('go', 488),\n ('life', 457),\n ('day', 455),\n ('time', 434),\n ('follow', 405),\n ('girl', 354),\n ('want', 336),\n ('know', 318),\n ('peopl', 316),\n ('live', 310),\n ('thing', 296),\n ('new', 292),\n ('best', 275),\n ('look', 271),\n ('good', 263),\n ('work', 252),\n ('world', 244),\n ('im', 232),\n ('got', 231),\n ('see', 228),\n ('need', 226),\n ('say', 226),\n ('think', 225),\n ('take', 216),\n ('friend', 214),\n ('year', 213),\n ('back', 213),\n ('come', 213),\n ('still', 211),\n ('happi', 208),\n ('feel', 208),\n ('amp', 203),\n ('music', 203),\n ('fuck', 197),\n ('alway', 196),\n ('even', 196),\n ('last', 196),\n ('thank', 194),\n ('tri', 192),\n ('lover', 191),\n ('god', 188),\n ('___', 188),\n ('right', 185),\n ('never', 182),\n ('way', 182)]\nBrand most common: 103144\n[('weather', 2327),\n ('get', 1513),\n ('updat', 1379),\n ('channel', 1218),\n ('15', 1195),\n ('40', 740),\n ('news', 604),\n ('39', 425),\n ('us', 421),\n ('follow', 419),\n ('new', 387),\n ('love', 313),\n ('make', 298),\n ('like', 295),\n ('tweet', 289),\n ('one', 281),\n ('best', 272),\n ('world', 266),\n ('time', 239),\n ('game', 239),\n ('help', 228),\n ('twitter', 222),\n ('free', 216),\n ('music', 207),\n ('day', 207),\n ('latest', 206),\n ('need', 201),\n ('look', 199),\n ('10', 199),\n ('servic', 196),\n ('find', 195),\n ('live', 195),\n ('offici', 194),\n ('year', 194),\n ('amp', 193),\n ('busi', 193),\n ('come', 187),\n ('see', 185),\n ('go', 184),\n ('life', 182),\n ('video', 179),\n ('peopl', 174),\n ('check', 169),\n ('play', 169),\n ('book', 168),\n ('account', 165),\n ('com', 162),\n ('sport', 157),\n ('know', 156),\n ('work', 154)]\n"
     ]
    }
   ],
   "source": [
    "count_country = Counter()\n",
    "count_country.update(locations)\n",
    "print(\"must popular country:\")\n",
    "print(count_country.most_common(2))\n",
    "\n",
    "count_male = Counter()\n",
    "count_male.update(terms_male)\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(\"Male most common:\")\n",
    "pp.pprint(count_male.most_common(50))\n",
    "\n",
    "\n",
    "count_female = Counter()\n",
    "count_female.update(terms_female)\n",
    "print(\"Female most common:\")\n",
    "pp.pprint(count_female.most_common(50))\n",
    "\n",
    "count_brand = Counter()\n",
    "count_brand.update(terms_brand)\n",
    "print(\"Brand most common:\")\n",
    "pp.pprint(count_brand.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Data to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ID                                               text\n0      815719226  sing rhythm 08c2c2 ffffff robbi respond critic...\n1      815719227  author novel fill famili drama romanc 0084b4 c...\n2      815719228  loui whine squeal abb8c2 c0deed absolut ador l...\n3      815719229  mobil guy 49er shazam googl kleiner perkin yah...\n4      815719230  ricki wilson best frontman kaiser chief best b...\n5      815719231  know f5abb5 ive seen peopl train lamp chair tv...\n6      815719232  global marketplac imag video music share photo...\n7      815719233  secret get ahead get start 0000ff c0deed gala ...\n8      815719234  pll fan crazi mcd ramen bae 9266cc @_aphmau_ p...\n9      815719235  renaiss art historian univers nottingham fuell...\n10     815719236  clean food tast great provid energi nutrient g...\n11     815719237  highli extraordinari auction 0084b4 c0deed mtg...\n12     815719238  senior 16 xi xii mmxiv 0084b4 ffffff put ass l...\n13     815719239  come join fastest blog network onlin today htt...\n14     815719240  im tp bo burnham disney world 0084b4 ffffff ev...\n15     815719241  0084b4 c0deed https://t.co/erogwtftyo glow sat...\n16     815719242  jmkm_ 58185 eeeeee @giannaaa28 lmao dude hella...\n17     815719243  enthusiast f1 fan model collector music fan fi...\n18     815719244  0084b4 c0deed @caribbro @jstsaleem understand ...\n19     815719245  0084b4 saw five lioness drink around littl pan...\n20     815719246  artisan special paper mach print make fibr art...\n21     815719247  bled die take away sin d41ebf 65b0da girl went...\n22     815719248  union xxxx 0084b4 c0deed @chrisaoffici right s...\n23     815719249  start 3b94d9 c0deed yall lmfaoo right choru ca...\n24     815719250  bsc econom graduat #coy 0084b4 c0deed jame bon...\n25     815719251  wife coach mom eight troop follow christ 0084b...\n26     815719252  question islam would like answer visit http://...\n27     815719253  14 canadian space enthusiast futur astronaut h...\n28     815719254  dm close sc dear_moonshin eea525 eea525 leav g...\n29     815719255  rl writer lewd aspir femboy enjoy oneechan gir...\n...          ...                                                ...\n20020  815754293  pacif victori roll websit #raaf #rnzaf pacif a...\n20021  815754330  camden nj singl ff8c00 ffffff ye im tote catch...\n20022  815754362  come let blow mind 3b94d9 think go move colora...\n20023  815754554  0084b4 c0deed georg bush dick cheney need vaca...\n20024  815754614  #code #linux #food oh #art 89c9fa droid look h...\n20025  815754750  promot market solut leisur tourism hospit life...\n20026  815755076  freelanc @halo writer work appear @haloforumeu...\n20027  815755206  0084b4 c0deed junior doctor civil unrest londo...\n20028  815755336  sigh 9266cc @lmaokaylinx @charlieputh open pic...\n20029  815755512  music sport photografi travel shop internet 00...\n20030  815755552  crowdsourc #innov finnd connect fed govt indus...\n20031  815755604  0084b4 c0deed bun save kid rib hero cape black...\n20032  815755689  amic person self indulg writer somehow licens ...\n20033  815755763  ci 18 volleybal enthusiast art sometim qualifi...\n20034  815755825  raleigh dynam leader come connect host work pl...\n20035  815755887  25 love friend famili color pink mkto follow 1...\n20036  815756011  13 snapchat sianfreya @caitlin_zz_ jack new ha...\n20037  815756103  typograph typograph consult co founder @fontsi...\n20038  815756269  fun site anim lover get latest anim news best ...\n20039  815756332  find wild thing 0084b4 c0deed especi best stil...\n20040  815756417  houston chronicl columnist ken hoffman @chron ...\n20041  815756542  famili go divorc help parent talk divorc child...\n20042  815756642  review delect #food picturesqu #travel kickass...\n20043  815756700  head chef chez bruce love car bicycl 90 food 0...\n20044  815756767  love 0084b4 c0deed need ride home practic ___a...\n20045  815757572  rp 0084b4 c0deed @lookupondeath fine drink tea...\n20046  815757681  whatev like problem 15 #chargern #foreverroy #...\n20047  815757830  #teambarcelona look lost follow follow heart b...\n20048  815757921  anti statist homeschool kid aspir thoughtlead ...\n20049  815757985  teamwork make dream work 0084b4 c0deed think a...\n\n[20050 rows x 2 columns]\n        gender\n0         male\n1         male\n2         male\n3         male\n4       female\n5       female\n6        brand\n7         male\n8       female\n9       female\n10       brand\n11       brand\n12      female\n13       brand\n14      female\n15      female\n16      female\n17        male\n18        male\n19     unknown\n20      female\n21      female\n22      female\n23        male\n24        male\n25      female\n26       brand\n27       brand\n28      female\n29        male\n...        ...\n20020    brand\n20021     male\n20022     male\n20023     male\n20024    brand\n20025    brand\n20026     male\n20027     male\n20028   female\n20029    brand\n20030    brand\n20031     male\n20032     male\n20033   female\n20034    brand\n20035   female\n20036   female\n20037     male\n20038    brand\n20039   female\n20040     male\n20041    brand\n20042    brand\n20043     male\n20044   female\n20045   female\n20046     male\n20047     male\n20048   female\n20049   female\n\n[20050 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "Table = Table.loc[Table['gender'].isin(['female','male'])]\n",
    "x = Table['text']\n",
    "y = Table[\"gender\"]\n",
    "\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20, shuffle=True)\n",
    "token = Tokenizer(num_words=5000)\n",
    "token.fit_on_texts(x)\n",
    "dic = token.word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and Train the keras deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to deep learning model\n",
    "Train = []\n",
    "Test = []\n",
    "for data in xTrain:\n",
    "    Train.append([dic[word] for word in kpt.text_to_word_sequence(data)])\n",
    "for data in xTest:\n",
    "    Test.append([dic[word] for word in kpt.text_to_word_sequence(data)])\n",
    "Train = np.asarray(Train)\n",
    "Test = np.asarray(Test)\n",
    "xTrainMatx = token.sequences_to_matrix(Train, mode='binary')\n",
    "xTestMatx = token.sequences_to_matrix(Test, mode='binary')\n",
    "lb_make = LabelEncoder()\n",
    "trainNum = lb_make.fit_transform(yTrain)\n",
    "testNum = lb_make.fit_transform(yTest)\n",
    "yTrainMatx = keras.utils.to_categorical(trainNum,2)\n",
    "yTestMatx = keras.utils.to_categorical(testNum,2)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(5000,), activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, input_shape=(5000,), activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, input_shape=(5000,), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(xTrainMatx, yTrainMatx,batch_size=128,epochs=5,validation_data=(xTestMatx, yTestMatx), verbose=1, shuffle=True)\n",
    "scores = model.evaluate(xTestMatx, yTestMatx, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Naive bayes and SVM models with Bag of words and TF-IDF.\n",
    "\n",
    "Show accuracy of all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results.append(['keras deep learning', scores[1]])\n",
    "def classify(parameters, feature, ml):\n",
    "            gs_clf = Pipeline(steps=[('vect', feature), ('clf', ml)])\n",
    "#             gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "            gs_clf = gs_clf.fit(xTrain, yTrain)\n",
    "            prediction = gs_clf.predict(xTest)\n",
    "            accuracy = metrics.accuracy_score(yTest, prediction)\n",
    "            return accuracy\n",
    "\n",
    "parameters = {'vect__max_df': (0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0), 'clf__alpha': (0.001, 0.01, 0.1, 1.0)} # understand why this numbers\n",
    "\n",
    "# Using classify with Bag of Words and SVM algorithm\n",
    "ans = classify(parameters, CountVectorizer(), SGDClassifier())\n",
    "results.append(['SVM with Bag of Words', ans])\n",
    "\n",
    "# Using classify with Bag of Words and NB algorithm\n",
    "ans = classify(parameters, CountVectorizer(), MultinomialNB())\n",
    "results.append(['Naïve Bayes with Bag of Words',  ans])\n",
    "\n",
    "# Using classify with tf-idf and SVM algorithm\n",
    "ans = classify(parameters, TfidfVectorizer(sublinear_tf=True, stop_words='english'), SGDClassifier())\n",
    "results.append(['SVM with TF-IDF', ans])\n",
    "\n",
    "# Using classify with tf-idf and NB algorithm\n",
    "ans = classify(parameters, TfidfVectorizer(sublinear_tf=True, stop_words='english'), MultinomialNB())\n",
    "results.append(['Naïve Bayes with TF-IDF', ans])\n",
    "\n",
    "accuracy = pd.DataFrame(results, columns=['machine learning algorithm','accuracy'])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(\"YzWFDST42FGUpQrqnFd6cT85u\", \"Ois5i7dau00xpluXjFPuU3Uu20UjDxd9KXbQjd7duXEO370iu8\")\n",
    "auth.set_access_token(\"1089235299862040576-gxsMbZg9nQgbooxCpsJ33ETDKb6IM5\", \"bSg2xKANcbIYhxEpL8OMwZIY7gXXbkc2wUkStxMSbU4K7\")\n",
    "api = tweepy.API(auth,  wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tweets by stremming api, with the most popular location of the train data.\n",
    "\n",
    "geo location is: -162.8,28.2,-64.4,71.6 (US & CANADA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    num_tweets = 0\n",
    "    def on_data(self, data):\n",
    "        if self.num_tweets < 15000:\n",
    "                try:\n",
    "                    with open('tweetsFromTwitter.json', 'a') as f:\n",
    "                        f.write(data)\n",
    "                        self.num_tweets += 1\n",
    "                        return True\n",
    "                except BaseException as e:\n",
    "                    print(\"Error on_data: %s\" % str(e))\n",
    "                    return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "    def _init_(self, api=None):\n",
    "        super(MyStreamListener, self)._init_()\n",
    "        self.num_tweets = 0\n",
    "\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth=api.auth, listener=myStreamListener)\n",
    "myStream.filter(locations=[-162.8,28.2,-64.4,71.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the tweets from 'tweetsFromTwitter.json' file.\n",
    "\n",
    "clean every tweet and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsTwitter = []\n",
    "for row in open('tweetsFromAPI.json','r'):\n",
    "    try:\n",
    "        tweet = json.loads(row)\n",
    "        cleanTweet = cleanAndNormalizeText(tweet['text'])\n",
    "        tweetsTwitter.append(cleanTweet)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show must popular words in the testing tweets from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most popular terms\n",
    "terms = []\n",
    "count = Counter()\n",
    "for tweet in tweetsTwitter:\n",
    "    for token in tweet:\n",
    "        terms.append(token)\n",
    "count.update(terms)\n",
    "print(count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a ovelap between the popular words in the train data and the tweets that we collected from twitter,\n",
    "\n",
    "we can asume that the overlap is indicate of common words in the lanuaze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 - Predict gender of authors with the best model from Q2.\n",
    "\n",
    "we can see that the Naive bayes with the TF-IDF bring the best accuracy ~(0.67)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict gender for all tweets that we collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for tweet in tweetsTwitter:\n",
    "    if len(tweet) > 0:\n",
    "        gs_clf = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, stop_words='english'), ), ('clf', MultinomialNB())])\n",
    "        # gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "        gs_clf = gs_clf.fit(xTrain, yTrain)\n",
    "        prediction = gs_clf.predict(tweet)\n",
    "        predct = (word for word in prediction)\n",
    "        c = Counter(predct)\n",
    "        predict = c.most_common(1)[0]\n",
    "        predictions.append([tweet, predict])\n",
    "\n",
    "Pred = pd.DataFrame(predictions,columns=['Tweet','Gender'])\n",
    "print(Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the prediction for all the tweets.\n",
    "    \n",
    "in every tweet we take the majority classification for all the terms in the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all terms prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, stop_words='english'), ), ('clf', MultinomialNB())])\n",
    "gs_clf = gs_clf.fit(xTrain, yTrain)\n",
    "prediction = gs_clf.predict(terms)\n",
    "maleC = np.count_nonzero(prediction == 'male')\n",
    "femaleC = np.count_nonzero(prediction == 'female')\n",
    "plt.pie([maleC, femaleC], explode=(0.1,0), labels=('Male','Female'),colors=['blue','red'],\n",
    "        autopct='%1.1f%%',shadow=True, startangle=180)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which gender have the most common words in there tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
